{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2.2\n",
    "En este ejercicio vamos a hacer scrapping sobre una página de meteorología para luego aplicar un simple modelo predictivo de Machine Learning\n",
    "\n",
    "En el ejercicio tendrás que programas parte de un código python. En concreto, las partes a programar tienen las siguientes características:\n",
    "+ Vienen precedidas por el comentario: __########INSERTAR CODIGO AQUI__. Eso quiere decir que la parte a programar será en la línea o líneas inmediatamente inferiores\n",
    "+ En concreto tendrás que rellenar aquellas partes en las que ponga la  palabra clave __None__ (elimina la palabra None y escribe tu código)\n",
    "+ Es posible que tengas que contestar a preguntas con texto. Para ello se han habilitado celdas de texto vacías, que podrás rellenar\n",
    "\n",
    "__Descripción__:\n",
    "La página https://www.tutiempo.net/clima/ws-82210.html contiene información histórica sobre distintos indicadores meteorológicos en una ubicación concreta, Madrid / Barajas.\n",
    "\n",
    "Nuestros objetivos van a ser:\n",
    "1. Hacer scraping sobre la página para obtener como resultado un dataframe de pandas (una tabla) que tenga todos los datos que hay en la web\n",
    "\n",
    "2. Limpiar y transformar los datos para que podamos hacer mineria de datos sobre ellos\n",
    "\n",
    "3. Análisis exploratorio de datos y detección de outilers\n",
    "\n",
    "4. Generacion de un pequeño sistema predictivo (clasificación) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Generación de nuestro dataset de trabajo\n",
    "* Vamos a empezar importando las librerias de numpy, pandas, matplotlib, requests y BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########INSERTAR CODIGO AQUI\n",
    "import None\n",
    "import None\n",
    "import None\n",
    "import None\n",
    "from None import None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación hacemos la petición al servidor de la web y vemos que el estado resultante es adecuado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos la petición mediante un get de requests\n",
    "########INSERTAR CODIGO AQUI\n",
    "res = None\n",
    "\n",
    "#Observamos el código de estado de nuestra petición\n",
    "########INSERTAR CODIGO AQUI\n",
    "print(f\"Tu peticion ha obtenido el código de estado: {res.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora vamos a usar BeautifulSoup para parsear el contenido de res. Usa el parser 'html'. Lo guardaremos en la variable soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parseamos el contenido de nuestra petición con BeautifulSoup, con parser tipo 'html'\n",
    "########INSERTAR CODIGO AQUI\n",
    "soup = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Antes de empezar, vamos a guardarnos en una variable el título de la web, ya que luego lo usaremos para nombrar nuestras tablas\n",
    "\n",
    "__TIP 1__: Busca con el inspector del navegador el nombre del tag del título ('Clima Madrid / Barajas Datos climáticos: xxx) y la clase que tiene asignada ese tag\n",
    "\n",
    "__TIP 2__: usa la función __.find('nombre_tag', class_='clase_del_tag')__ de BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos la información del tag\n",
    "########INSERTAR CODIGO AQUI\n",
    "div_titulo = None\n",
    "\n",
    "#Obtenemos el texto en dicho tag\n",
    "########INSERTAR CODIGO AQUI\n",
    "titulo = None\n",
    "\n",
    "print(f'El titulo de la página es: {titulo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A continuación vamos a pasar la tabla de datos a un dataframe de pandas.\n",
    "1. Vamos a guardar todos los datos del tag 'table'. Usa para ello la función __find__ de BeautifulSoup, teniendo en cuenta la clase de ese tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accedemos al tag de tipo 'table', teniendo en cuenta su clase\n",
    "########INSERTAR CODIGO AQUI\n",
    "tag_table = None\n",
    "\n",
    "print('Las dos primeras entradas de la tabla son:\\n')\n",
    "display(tag_table.find_all(\"tr\")[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Almacenamos la información del tag de la tabla en un dataframe de Pandas y visualizamos su dimension y las 5 primeras filas\n",
    "\n",
    "__TIP 1:__ Para crear el data frame usa la función de pandas __read_html(str(tag))[0]__\n",
    "\n",
    "__TIP 2:__ La dimension de un dataframe, al igual que de un array, se puede ver con .shape\n",
    "\n",
    "__TIP 2:__ Para visualizar las primeras filas, usa una función de pandas, no un bucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el dataframe (tabla de datos) a partir del tag\n",
    "########INSERTAR CODIGO AQUI\n",
    "df = None\n",
    "\n",
    "#Vemos la dimensión de nuestra tabla\n",
    "########INSERTAR CODIGO AQUI\n",
    "print(f'Nuestra tabla tiene esta dimension: {None}')\n",
    "\n",
    "#Visualizamos las primeras 5 muestras de nuestro dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta__: Si vamos a usar estos datos para un sistema de Machine Learning: 1) ¿Hasta cuantas dimensiones crees que podría tener cada muestra?, 2) ¿Qué tamaño (número de muestras) tendría nuestro dataset inicial? Responde brevemente pero de forma razonada a ambas preguntas en la siguiente celda.\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Análisis preliminar, limpieza y transformación de nuestro dataset\n",
    "\n",
    "* Empezamos haciendo un doble análisis de los datos. Primero, siendo que no hay muchas filas, vamos a analizar si hay filas con datos faltantes (__TIP__: Busca las filas que tienen algún '-' y apunta si índice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vamos a limpiar los datos:\n",
    "1. Generamos una lista en python con los índices de las filas con datos faltantes (Ejemplo de lista l=[elemento_1, elemento_2, ..., elemento_n])\n",
    "2. Eliminamos del dataframe las filas identificadas como malas (con datos faltantes). El resultado lo guardamos en un segundo dataframe\n",
    "\n",
    "__TIP:__ Para eliminar filas de un dataframe usaremos la función __.drop(lista_filas_a_eliminar, axis = 0, inplace = False)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos una lista con los índices que queremos borrar\n",
    "########INSERTAR CODIGO AQUI\n",
    "row_indexes_of_missing_data = None\n",
    "\n",
    "#Eliminamos de nuestro dataframe las filas contenidas en la lista anterior\n",
    "########INSERTAR CODIGO AQUI\n",
    "df_cleaned = None\n",
    "\n",
    "print(\"Mis datos limpios son:\\n\")\n",
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta__: ¿Con lo que sabemos ahora, cambiarías tu respuesta sobre el número de dimensiones o el número de muestras útiles en nuestro dataset? Responde brevemente en la siguiente celda\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ahora vamos a transformar los datos. Para ello vamos a renombrar las columnas para poner un nombre más inteligible, luego veremos en qué formato están los datos, y finalmente, si lo vemos conveniente, los transformaremos a formato numérico\n",
    "\n",
    "1. Vamos a convertir los nombres (headers) de nuestras columnas del dataframe. Primero hacemos una copia de nuestro dataframe (__funcion .copy()__), luego sustituimos los antiguos nombres de columna por los nuevos (usaremos el atributo df.columns y le asignaremos los nuevos nombres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos los nuevos nombres de columnas\n",
    "new_headers = ['Año', 'TempMedia', 'TempMax', 'Tempmin', 'Precipitacion',\n",
    "               'VelocidadViento', 'DiasLluvia', 'DiasNieve', 'DiasTormenta',\n",
    "              'DiasNiebla', 'DiasTornado', 'DiasGranizo']\n",
    "\n",
    "#Clonamos el dataframe de datos limpiados y lo guardamos en una nueva variable\n",
    "########INSERTAR CODIGO AQUI\n",
    "df_transformed = None\n",
    "\n",
    "#Asignamos al atributo .columns de nuestro nuevo dataframe los nombres que hemos definido\n",
    "########INSERTAR CODIGO AQUI\n",
    "df_transformed.columns = None\n",
    "\n",
    "print(\"Comprobacion. Mi dataset tiene ahora estas columnas:\")\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vemos la información de nuestro dataframe. ¿Están nuestros datos ya en formato numérico? Responde brevemente y de forma razonada en la celda habilitada para ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transformamos las columnas a formato numérico. Para ello iremos columna a columna (sólo aquellas que creamos convenientes) y le iremos cambiando el tipo de datos. Finalmente, comprueba de alguna forma que la transformación se ha producido\n",
    "\n",
    "__TIP 1:__ Se puede cambiar el tipo de todos los datos de una columna de un dataframe usando __df['nombre_columna'] = df['nombre_columna'].astype(tipo_salida)__\n",
    "\n",
    "__TIP 2:__ Podemos ir una a una por todas las columnas (1 línea de código por columna cambiada), o bien podemos hacer un bucle _for_, ya que tenemos todos los nombres de las columnas en una variable. Recuerda si haces esto, que igual nos interesa usar rangos dentro la lista de nombres [valor_inicial:valor_final:paso]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos los datos (elige: columna a columna o mediante bucle for)\n",
    "########INSERTAR CODIGO AQUI (son múltiples líneas de código)\n",
    "None \n",
    "\n",
    "#Comprobamos si la transformación ha tenido lugar\n",
    "df_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta__: ¿Se ha producido la transformación? Explícalo en una línea en la siguiente celda\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Análisis exploratorio e identificación de outliers (visual y con ML)\n",
    "\n",
    "* Vamos a empezar visualizando los datos más importantes: Temperatura Media y Precipitaciones anuales. En ambos casos, el eje x serán los años\n",
    "\n",
    "1. Convierte las columnas Año, TempMedia y Precipitacion de nuestro dataframe cada una a un array de numpy\n",
    "\n",
    "__TIP__: Usa la función de pandas df['columna'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos cada columna a un array de numpy\n",
    "########INSERTAR CODIGO AQUI\n",
    "year_array = None\n",
    "temp_array = None\n",
    "rain_array = None\n",
    "\n",
    "#Comprobamos los resultados\n",
    "print(f\"Primeros cinco años: {year_array[:5]}\")\n",
    "print(f\"Primeras cinco Temp. Medias: {temp_array[:5]}\")\n",
    "print(f\"Primeras cinco Precipitaciones anuales: {rain_array[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Análisis exploratorio de datos (EDA). Vamos a visualizar en un mismo gráfico la evolución de la temperatura media y la precipitación anual. __Importante: la precipitación actualmente está en mm, pero por comodidad, la querremos visualizar como dm, es decir, dividiremos el array/100__\n",
    "Para visualiar usaremos el plot de matplotlib, y haremos que la temperatura se marque como una línea continua roja, mientras que la precipitación se marcará como una línea azul, discontinua (a rayas intermintentes) y que cada punto anual se marque con un círculo. Además, a la gráfica le pondremos como título, el título de la página web (lo tenemos en una variable), pondremos un grid y pondremos una leyenda para saber qué es cada curva \n",
    "\n",
    "__TIP 1:__ En la prácitca de introducción a matplotlib se vió cómo formatear las curvas en un plot de matplotlib (colores, puntos, tipo de línea)\n",
    "\n",
    "__TIP 2:__ Para que la leyenda tenga sentido, habrá que utilizar la etiqueta __label='nombre_de_curva'__ en el plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########INSERTAR CODIGO AQUI\n",
    "plt.plot(None, None, 'None', label = None)\n",
    "plt.plot(None, None/100, 'None', label = None)\n",
    "plt.title(None)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Serías capaz de identificar outliers en la serie anterior, combinando precipitación y temperatura? Da tu respuesta en un par de líneas en la siguiente celda.\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vamos a hacer ahora un análisis de outliers tanto basado en el análisis visual de los datos, como en el análisis con Machine Learning\n",
    "\n",
    "1. Vamos a visualizar los datos conjuntos de Temperatura (eje x) y Precipitación (eje y), usando el scatterplot de matplotlib. Le pondremos título a la gráfica (el título de la web), etiqueta de eje x, etiqueta de eje y, y grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########INSERTAR CODIGO AQUI\n",
    "plt.scatter(None,None)\n",
    "plt.title(None)\n",
    "plt.xlabel('Temperatura Media')\n",
    "plt.ylabel('Precipitacion Total')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Viendo la gráfica de puntos, serías ahora capaz de identificar mejor los outliers? Señala (alude a sus valores para referirte a ellos) los 5 puntos que más te parezcan outliers y explica en un par de líneas por qué crees que lo son.\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vamos a aplicar un sistema de Machine Learning de clustering, para ver si estadísticamente considera los mismos outliers que hemos considerado nosotros, teniendo en cuenta igualmente la temperatura y la precipitación anual. Tendremos que normalizar los datos antes.\n",
    "\n",
    "__a.__ Creamos la matriz (array 2d de numpy) de datos a partir del dataframe de datos transformados y la normalizamos utilizando el objeto StandardScaler del paquete preprocessing de sklearn\n",
    "\n",
    "__TIP__: Puedes normalizar los datos usando la función __fit_transform(datos)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos nuestra matriz de datos de 47 muestras y 2 dimensiones\n",
    "data = df_transformed[['TempMedia', 'Precipitacion']].to_numpy()\n",
    "########INSERTAR CODIGO AQUI\n",
    "print(f\"El tamaño de nuestros datos es: {None}\")\n",
    "\n",
    "#Importamos la clase StandardScaler del paquete preprocessing de sklearn\n",
    "########INSERTAR CODIGO AQUI\n",
    "from None import None\n",
    "\n",
    "#Generamos una variable de tipo StandardScaler y con ella normalizamos los datos\n",
    "########INSERTAR CODIGO AQUI\n",
    "standard_scaler = None\n",
    "data_scaled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b.__ Importamos de la libreria sklearn, del paquete cluster, la clase AgglomerativeClustering. Creamos un objeto de tipo AgglomerativeClustering que tenga como hiperparámetros (parámetros de inicialización): n_culsters = 7, affinity = 'euclidean', linkage = 'single'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la clase AgglomerativeClustering\n",
    "########INSERTAR CODIGO AQUI\n",
    "from None import None\n",
    "\n",
    "#Instanciamos un objeto de tipo AgglomerativeClustering con 3 hiperparámetros\n",
    "########INSERTAR CODIGO AQUI\n",
    "ag_clustering_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c.__ Entrenamos el modelo con nuestros datos y predecimos una etiqueta para cada uno de ellos\n",
    "\n",
    "__TIP:__ Puedes hacer las dos operaciones en una si usas __.fit_predict()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos nuestro modelo de clustering y predecimos para nuestros datos escalados\n",
    "predicted_labels_from_clustering = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d.__ Hacemos el scatterplot de matplotlib de los datos, y en este caso le ponemos color según las etiquetas predichas y el mapa de colores cmap='viridis'.Luego generamos una barra de colores (__colorbar()__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "########INSERTAR CODIGO AQUI\n",
    "plt.scatter(data[:,0], data[:,1], c=None, cmap=None)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Viendo la gráfica de puntos generada y la distribución de los clusters (distintos colores), crees que el sistema ha agrupado las muestras de la misma forma que lo habrías hecho tú? ¿Crees que el sistema ha identificado los mismos outliers que tú habías identificado antes? Responde brevemente a estas preguntas en la siguiente celda.\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Generación de un pequeño sistema predictivo\n",
    "\n",
    "Para finalizar esta actividad, vamos a intentar generar un pequeño sistema predictivo. Hemos de ser conscientes de que la cantidad de datos en nuestro dataset es extremadamente pequeña (menos de 50 muestras), y por tanto no podremos conseguir buenos resultados.\n",
    "\n",
    "En concreto vamos a intentar hacer un clasificador binario, que dados los datos meteorológicos de un año, va a intentar predecir si ese día ha habido múltiples días de granizo o no. Al ser un clasificador binario, sabemos que si la tasa de acierto es cercana al 50%, no habremos conseguido nada (misma probabilidad de acertar que de fallar). En cambio, cualquier tasa de acierto que esté por encima ya será un éxito.\n",
    "\n",
    "Planteamos este ejercicio en dos pasos:\n",
    "\n",
    "__Paso 1.__ Generación de un clasificador directamente sobre los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generamos un nuevo dataframe que sea copia del dataframe de datos transformados (usas __.copy()__). A continuación visualizamos el histograma de días con granizo para determinar en promedio qué deberíamos considerar un _año con poco granizo_ frente a un _año con mucho granizo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos un dataframe copia del anterior\n",
    "########INSERTAR CODIGO AQUI\n",
    "df_ml = None\n",
    "\n",
    "#mostramos el histograma de días de granizo\n",
    "plt.hist(df_ml['DiasGranizo'])\n",
    "plt.xlabel('Dias de granizo en un año')\n",
    "plt.ylabel('Numero de muestras')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A la vista de los resultados del histograma, podríamos decir que hay una separación entre los años que hay 0 o 1 día de granizo, y los años que tienen más de un día de granizo. Por lo tanto, generamos nuestra columna objetivo, que tendrá dos posibles valores de clase:\n",
    "\n",
    "0- _Año con poco granizo_. Si en ese año ha habido 1 o menos días de granizo\n",
    "\n",
    "1- _Año con mucho granizo_. Si en ese año ha habido 2 o más días de granizo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos una Serie de tipo binario según el criterio de antes para días de granizo\n",
    "listado_binario_granizo = (df_transformed['DiasGranizo'] > 1).astype(int)\n",
    "display(listado_binario_granizo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añadimos una nueva columna a nuestro dataframe con ese listado\n",
    "########INSERTAR CODIGO AQUI\n",
    "df_ml['Año_granizo'] = None\n",
    "display(df_ml.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generamos nuestra matriz (array 2d de numpy) de muestras X, con todos los datos excepto el año (en realidad se trata como un índice) y excepto las columnas referentes a granizo (es lo que queremos predecir). A continuación generamos nuestro vector y (array de numpy) a partir de la columna 'Año_granizo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos los datos X\n",
    "X = df_ml[['TempMedia', 'TempMax', 'Tempmin', 'VelocidadViento','DiasLluvia',\n",
    "           'DiasNieve', 'DiasTormenta', 'DiasNiebla', 'DiasTornado']].to_numpy()\n",
    "\n",
    "#Generamos los targets y\n",
    "########INSERTAR CODIGO AQUI\n",
    "y = None\n",
    "\n",
    "print(f\"Tamaño de datos X: {X.shape}\")\n",
    "print(f\"Tamaño de etiquetas y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Segmentamos nuestro dataset en dos, datos de entrenamiento y datos de test. Para ello utilizamos la función __train_test_split__ de sklearn (del paquete model_selection). Dados los pocos datos que tenemos, hemos decidido dividir en 70% datos para entrenamiento y 30% para test. En la función train_test_split usaremos los siguientes parámetros de función: test_size = 0.30 y random_state = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la funcion train_test_split del paquete model_selection de sklearn\n",
    "########INSERTAR CODIGO AQUI\n",
    "from None import None\n",
    "\n",
    "#Hacemos la segmentación de datos considerando 30% de datos para test y random_state = 22\n",
    "########INSERTAR CODIGO AQUI\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "print(f\"Tamaño dataset entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño dataset test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generamos un clasificador de tipo DecisionTreeClassifier del paquete tree de sklearn sin hiperparámetros, luego entrenamos con nuestros datos de entrenamiento y hacemos predicciones sobre nuestros datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la clase DecisionTreeClassifier del paquete tree de sklearn\n",
    "########INSERTAR CODIGO AQUI\n",
    "from None import None\n",
    "\n",
    "#Generamos una instancia de DecisionTreeClassifier\n",
    "########INSERTAR CODIGO AQUI\n",
    "classifier_tree = None\n",
    "\n",
    "#Entrenamos sobre nuestras muestras de entrenamiento y nuestras etiquetas de entrenamiento\n",
    "########INSERTAR CODIGO AQUI\n",
    "classifier_tree.None\n",
    "\n",
    "#Hacemos una predicción de etiquetas sobre nuestros datos de test\n",
    "########INSERTAR CODIGO AQUI\n",
    "y_predict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtenemos métricas y la matriz de confusión de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "accuracy = accuracy_score(y_test,y_predict)\n",
    "print(f'Ratio de acierto: {accuracy}')\n",
    "print(f'Matriz de confusion:\\n {cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pregunta:__ ¿Podrías explicar los resultados obtenidos? ¿Sabiendo que teníamos pocas muestras, consideras que son buenos resultados? Responde en la siguente celda\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Paso 2.__ Generación de un clasificador aplicando normalización de datos y reducción de dimensiones\n",
    "\n",
    "Finalmente vamos a repetir el modelo predictivo anterior, pero en este caso vamos a realizar una normalización previa de los datos con StandardScaler, y posteriormente haremos una reducción de dimensiones con PCA.\n",
    "\n",
    "* Generamos una variable de tipo StandardScaler y normalizamos nuestros datos X (las etiquetas y nunca se normalizan). Para ello utilizaremos la función __fit_transform(datos)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "\n",
    "########INSERTAR CODIGO AQUI\n",
    "X_scaled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importamos la clase PCA del paquete decomposition de sklearn. La instanciamos con el hiperparámetro __n_components = 0.99__ (eso quiere decir que reduciremos dimensiones, pero quedándonos con el 99% de la información inicial). Reducimos las dimensiones de nuestros datos escalados (para ello utilizaremos la función __fit_transform(datos)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la clase PCA del paquete decomposition de sklearn\n",
    "from None import None\n",
    "\n",
    "#Instanciamos una variable de tipo PCA, con hiperparámetro n_components=0.99\n",
    "pca = None\n",
    "\n",
    "#Calculamos PCA sobre los datos escalados y lo aplicamos sobre los mismos para reducir dimensiones\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Ahora el tamaño de nuestro dataset es:{X_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo se ha reducido una dimensión (hemos pasado de 9 a 8 dimensiones). ¿Será suficiente?\n",
    "\n",
    "* Volvemos a segmentar los datos en entranamiento y test, pero en este caso sobre X_reduced en lugar de sobre X (misma cantidad de datos pero menos dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.30, random_state = 22)\n",
    "print(f\"Tamaño dataset entrenamiento - Tras reducción de dimensiones: {X_train_reduced.shape}\")\n",
    "print(f\"Tamaño dataset test - Tras reducción de dimensiones: {X_test_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generamos un clasificador de tipo DecisionTreeClassifier sin hiperparámetros y entrenamos nuestros datos de entrenamiento reducidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instancia una variable de tipo DecisionTreeClassifier()\n",
    "classifier_tree = DecisionTreeClassifier()\n",
    "\n",
    "#Entrena con los datos del dataset de entrenamiento de datos reducidos\n",
    "classifier_tree.fit(X_train_reduced, y_train)\n",
    "\n",
    "#Predice las etiquetas de los datos de test reducidos\n",
    "########INSERTAR CODIGO AQUI\n",
    "y_predict = classifier_tree.None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_reduced = confusion_matrix(y_test, y_predict)\n",
    "accuracy_reduced = accuracy_score(y_test,y_predict)\n",
    "print(f'Ratio de acierto con reduccion de dimensiones: {accuracy_reduced}')\n",
    "print(f'Matriz de confusion con reduccion de dimensiones:\\n {cm_reduced}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CONCLUSIÓN:__ Compara los resultados obtenidos al hacer el clasificador sobre los datos en bruto y sobre los datos preprocesados (normalizados y con reducción de dimensiones). ¿En algún caso el resultado es bueno? Analiza el resultado en un par de párrafos en la siguiente celda\n",
    "\n",
    "__A CONTINUACIÓN CELDA A RELLENAR POR EL ALUMNO__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
